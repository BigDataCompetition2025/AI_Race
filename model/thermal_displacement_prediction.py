"""
è»ŠåºŠç†±è®Šä½é æ¸¬æ¨¡å‹
ç›®æ¨™ï¼šåŸºæ–¼æº«åº¦åŠè®Šä½è³‡è¨Šå»ºç«‹è»ŠåºŠç†±è®Šä½é æ¸¬æ¨¡å‹

è¼¸å…¥ç‰¹å¾µ (25 dim):
- æ™‚é–“ (1 dim): Time
- è»ŠåºŠä½ç½®æº«åº¦ (21 dim): PT01-PT13 + TC01-TC08
- æ§åˆ¶å™¨æ“·å–æº«åº¦ (3 dim): Spindle Motor, X Motor, Z Motor

è¼¸å‡º:
- Xè»¸è®Šä½é‡: Disp. X
- Zè»¸è®Šä½é‡: Disp. Z

è©•ä¼°æ–¹å¼: RMSE
è¨“ç·´é›†:æ¸¬è©¦é›† = 77.03:22.97
"""

import pandas as pd
import numpy as np
import os
import glob
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import cross_val_score
import warnings
warnings.filterwarnings('ignore')

# (Removed hardcoded Windows font to stop findfont warnings on macOS/Linux)
plt.rcParams['axes.unicode_minus'] = False

class ThermalDisplacementPredictor:
    def __init__(self, data_path):
        self.data_path = data_path
        self.scaler_X = StandardScaler()
        self.scaler_y = StandardScaler()
        self.model_X = None
        self.model_Z = None
        self.feature_columns = None
        self.target_columns = ['Disp. X', 'Disp. Z']
        
    def load_data(self):
        """è¼‰å…¥æ‰€æœ‰CSVæª”æ¡ˆä¸¦åˆä½µ"""
        print("é–‹å§‹è¼‰å…¥è³‡æ–™...")
        
        # å–å¾—æ‰€æœ‰CSVæª”æ¡ˆè·¯å¾‘
        csv_files = glob.glob(os.path.join(self.data_path, "*.csv"))
        print(f"æ‰¾åˆ° {len(csv_files)} å€‹CSVæª”æ¡ˆ")
        
        all_data = []
        
        for file_path in csv_files:
            try:
                df = pd.read_csv(file_path)
                print(f"è¼‰å…¥æª”æ¡ˆ: {os.path.basename(file_path)}, è³‡æ–™ç­†æ•¸: {len(df)}")
                all_data.append(df)
            except Exception as e:
                print(f"è¼‰å…¥æª”æ¡ˆ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        # åˆä½µæ‰€æœ‰è³‡æ–™
        self.data = pd.concat(all_data, ignore_index=True)
        print(f"ç¸½è³‡æ–™ç­†æ•¸: {len(self.data)}")
        
        return self.data
    
    def explore_data(self):
        """è³‡æ–™æ¢ç´¢åˆ†æ"""
        print("\n=== è³‡æ–™æ¢ç´¢åˆ†æ ===")
        print(f"è³‡æ–™å½¢ç‹€: {self.data.shape}")
        print(f"\næ¬„ä½åç¨±:")
        print(self.data.columns.tolist())
        
        # æª¢æŸ¥ç¼ºå¤±å€¼
        print(f"\nç¼ºå¤±å€¼çµ±è¨ˆ:")
        missing_data = self.data.isnull().sum()
        print(missing_data[missing_data > 0])
        
        # åŸºæœ¬çµ±è¨ˆè³‡è¨Š
        print(f"\nç›®æ¨™è®Šæ•¸çµ±è¨ˆ:")
        print(self.data[self.target_columns].describe())
        
        # ç¹ªè£½ç›®æ¨™è®Šæ•¸åˆ†å¸ƒåœ–
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        
        axes[0].hist(self.data['Disp. X'], bins=50, alpha=0.7)
        axes[0].set_title('Disp. X Distribution')
        axes[0].set_xlabel('Disp. X')
        axes[0].set_ylabel('Frequency')
        
        axes[1].hist(self.data['Disp. Z'], bins=50, alpha=0.7)
        axes[1].set_title('Disp. Z Distribution')
        axes[1].set_xlabel('Disp. Z')
        axes[1].set_ylabel('Frequency')
        
        plt.tight_layout()
        plt.savefig('displacement_distribution.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return self.data.describe()
    
    def prepare_features(self):
        """æº–å‚™ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸"""
        print("\n=== æº–å‚™ç‰¹å¾µè³‡æ–™ ===")
        
        # å®šç¾©ç‰¹å¾µæ¬„ä½
        # æ™‚é–“ç‰¹å¾µ (1 dim)
        time_features = ['Time']
        
        # è»ŠåºŠä½ç½®æº«åº¦ (21 dim)
        pt_features = [f'PT{i:02d}' for i in range(1, 14)]  # PT01-PT13 (13å€‹)
        tc_features = [f'TC{i:02d}' for i in range(1, 9)]   # TC01-TC08 (8å€‹)
        position_temp_features = pt_features + tc_features
        
        # æ§åˆ¶å™¨æ“·å–æº«åº¦ (3 dim)
        motor_temp_features = ['Spindle Motor', 'X Motor', 'Z Motor']
        
        # åˆä½µæ‰€æœ‰è¼¸å…¥ç‰¹å¾µ
        self.feature_columns = time_features + position_temp_features + motor_temp_features
        
        print(f"è¼¸å…¥ç‰¹å¾µæ•¸é‡: {len(self.feature_columns)}")
        print(f"æ™‚é–“ç‰¹å¾µ (1): {time_features}")
        print(f"ä½ç½®æº«åº¦ç‰¹å¾µ (21): {position_temp_features}")
        print(f"é¦¬é”æº«åº¦ç‰¹å¾µ (3): {motor_temp_features}")
        
        # æª¢æŸ¥ç‰¹å¾µæ˜¯å¦å­˜åœ¨
        missing_features = [col for col in self.feature_columns if col not in self.data.columns]
        if missing_features:
            print(f"è­¦å‘Š: ä»¥ä¸‹ç‰¹å¾µåœ¨è³‡æ–™ä¸­ä¸å­˜åœ¨: {missing_features}")
            self.feature_columns = [col for col in self.feature_columns if col in self.data.columns]
            print(f"å¯¦éš›å¯ç”¨ç‰¹å¾µæ•¸é‡: {len(self.feature_columns)}")
        
        return self.feature_columns
    
    def handle_missing_data(self):
        """è™•ç†ç¼ºå¤±è³‡æ–™"""
        print("\n=== è™•ç†ç¼ºå¤±è³‡æ–™ ===")
        
        # æª¢æŸ¥ç¼ºå¤±å€¼
        missing_counts = self.data[self.feature_columns + self.target_columns].isnull().sum()
        print("å„æ¬„ä½ç¼ºå¤±å€¼æ•¸é‡:")
        print(missing_counts[missing_counts > 0])
        
        # ä½¿ç”¨ç·šæ€§æ’å€¼å¡«è£œç¼ºå¤±å€¼
        for col in self.feature_columns + self.target_columns:
            if self.data[col].isnull().sum() > 0:
                print(f"å° {col} é€²è¡Œç·šæ€§æ’å€¼")
                self.data[col] = self.data[col].interpolate(method='linear')
                
                # å¦‚æœé¦–å°¾ä»æœ‰ç¼ºå¤±å€¼ï¼Œä½¿ç”¨å‰å¾Œå¡«è£œ
                self.data[col] = self.data[col].fillna(method='bfill').fillna(method='ffill')
        
        # å†æ¬¡æª¢æŸ¥ç¼ºå¤±å€¼
        final_missing = self.data[self.feature_columns + self.target_columns].isnull().sum().sum()
        print(f"è™•ç†å¾Œå‰©é¤˜ç¼ºå¤±å€¼æ•¸é‡: {final_missing}")
        
        return self.data
    
    def split_data(self, test_size=0.2297):  # 77.03:22.97
        """åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›†"""
        print(f"\n=== è³‡æ–™åˆ†å‰² ===")
        
        X = self.data[self.feature_columns]
        y = self.data[self.target_columns]
        
        # åˆ†å‰²è³‡æ–™
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, shuffle=True
        )
        
        print(f"è¨“ç·´é›†å¤§å°: {len(self.X_train)} ({len(self.X_train)/len(X)*100:.2f}%)")
        print(f"æ¸¬è©¦é›†å¤§å°: {len(self.X_test)} ({len(self.X_test)/len(X)*100:.2f}%)")
        
        return self.X_train, self.X_test, self.y_train, self.y_test
    
    def train_model(self, model_type='linear'):
        """è¨“ç·´æ¨¡å‹"""
        print(f"\n=== è¨“ç·´æ¨¡å‹ ({model_type}) ===")
        
        # æ¨™æº–åŒ–ç‰¹å¾µ
        X_train_scaled = self.scaler_X.fit_transform(self.X_train)
        
        if model_type == 'linear':
            # ç·šæ€§å›æ­¸æ¨¡å‹
            self.model_X = LinearRegression()
            self.model_Z = LinearRegression()
        elif model_type == 'random_forest':
            # éš¨æ©Ÿæ£®æ—æ¨¡å‹
            self.model_X = RandomForestRegressor(n_estimators=100, random_state=42)
            self.model_Z = RandomForestRegressor(n_estimators=100, random_state=42)
        elif model_type == 'gradient_boosting':
            # æ¢¯åº¦æå‡æ¨¡å‹
            self.model_X = GradientBoostingRegressor(
                n_estimators=100, 
                learning_rate=0.1, 
                max_depth=6,
                random_state=42
            )
            self.model_Z = GradientBoostingRegressor(
                n_estimators=100, 
                learning_rate=0.1, 
                max_depth=6,
                random_state=42
            )
        
        # åˆ†åˆ¥è¨“ç·´Xè»¸å’ŒZè»¸çš„æ¨¡å‹
        print("è¨“ç·´ Xè»¸è®Šä½é æ¸¬æ¨¡å‹...")
        self.model_X.fit(X_train_scaled, self.y_train['Disp. X'])
        
        print("è¨“ç·´ Zè»¸è®Šä½é æ¸¬æ¨¡å‹...")
        self.model_Z.fit(X_train_scaled, self.y_train['Disp. Z'])
        
        print("æ¨¡å‹è¨“ç·´å®Œæˆï¼")
        
        return self.model_X, self.model_Z
    
    def evaluate_model(self):
        """è©•ä¼°æ¨¡å‹"""
        print("\n=== æ¨¡å‹è©•ä¼° ===")
        
        # æ¨™æº–åŒ–æ¸¬è©¦é›†
        X_test_scaled = self.scaler_X.transform(self.X_test)
        
        # é æ¸¬
        y_pred_X = self.model_X.predict(X_test_scaled)
        y_pred_Z = self.model_Z.predict(X_test_scaled)
        
        # è¨ˆç®—æ•´é«”RMSE
        rmse_X = np.sqrt(mean_squared_error(self.y_test['Disp. X'], y_pred_X))
        rmse_Z = np.sqrt(mean_squared_error(self.y_test['Disp. Z'], y_pred_Z))
        
        print(f"æ•´é«” Xè»¸è®Šä½ RMSE: {rmse_X:.6f}")
        print(f"æ•´é«” Zè»¸è®Šä½ RMSE: {rmse_Z:.6f}")
        print(f"æ•´é«”å¹³å‡ RMSE: {(rmse_X + rmse_Z) / 2:.6f}")
        
        # è¨ˆç®—RÂ²åˆ†æ•¸
        from sklearn.metrics import r2_score
        r2_X = r2_score(self.y_test['Disp. X'], y_pred_X)
        r2_Z = r2_score(self.y_test['Disp. Z'], y_pred_Z)
        
        print(f"Xè»¸è®Šä½ RÂ²: {r2_X:.6f}")
        print(f"Zè»¸è®Šä½ RÂ²: {r2_Z:.6f}")
        
        # åˆ†å€RMSEåˆ†æ
        segmented_results = self.segmented_rmse_analysis(y_pred_X, y_pred_Z)
        
        # ç¹ªè£½é æ¸¬çµæœå°æ¯”åœ–
        self.plot_predictions(y_pred_X, y_pred_Z)
        
        return {
            'rmse_X': rmse_X,
            'rmse_Z': rmse_Z,
            'avg_rmse': (rmse_X + rmse_Z) / 2,
            'r2_X': r2_X,
            'r2_Z': r2_Z,
            'segmented_results': segmented_results
        }
    
    def segmented_rmse_analysis(self, y_pred_X, y_pred_Z):
        """åˆ†å€RMSEåˆ†æ - è­˜åˆ¥æ¥µç«¯æƒ…æ³å’Œä¸åŒæ¢ä»¶ä¸‹çš„æ¨¡å‹è¡¨ç¾"""
        print("\n=== åˆ†å€RMSEåˆ†æ ===")
        
        # å‰µå»ºçµæœå­—å…¸
        segmented_results = {}
        
        # 1. æŒ‰è®Šä½å¤§å°åˆ†å€åˆ†æ
        print("\n1. æŒ‰è®Šä½å¤§å°åˆ†å€:")
        segmented_results['displacement_ranges'] = self._analyze_by_displacement_range(y_pred_X, y_pred_Z)
        
        # 2. æŒ‰æº«åº¦ç¯„åœåˆ†å€åˆ†æ
        print("\n2. æŒ‰ä¸»è»¸é¦¬é”æº«åº¦åˆ†å€:")
        segmented_results['temperature_ranges'] = self._analyze_by_temperature_range(y_pred_X, y_pred_Z)
        
        # 3. æŒ‰æ™‚é–“åˆ†å€åˆ†æ
        print("\n3. æŒ‰æ™‚é–“åˆ†å€:")
        segmented_results['time_ranges'] = self._analyze_by_time_range(y_pred_X, y_pred_Z)
        
        # 4. æ¥µç«¯å€¼åˆ†æ
        print("\n4. æ¥µç«¯å€¼åˆ†æ:")
        segmented_results['extreme_cases'] = self._analyze_extreme_cases(y_pred_X, y_pred_Z)
        
        return segmented_results
    
    def _analyze_by_displacement_range(self, y_pred_X, y_pred_Z):
        """æŒ‰è®Šä½å¤§å°åˆ†å€åˆ†æ"""
        ranges_results = {}
        
        for axis, actual, pred in [('X', self.y_test['Disp. X'], y_pred_X), 
                                   ('Z', self.y_test['Disp. Z'], y_pred_Z)]:
            # å®šç¾©è®Šä½å¤§å°å€é–“
            percentiles = [0, 25, 50, 75, 100]
            range_boundaries = np.percentile(actual, percentiles)
            
            axis_results = []
            for i in range(len(percentiles)-1):
                mask = (actual >= range_boundaries[i]) & (actual < range_boundaries[i+1])
                if i == len(percentiles)-2:  # æœ€å¾Œä¸€å€‹å€é–“åŒ…å«æœ€å¤§å€¼
                    mask = actual >= range_boundaries[i]
                
                if mask.sum() > 0:
                    rmse = np.sqrt(mean_squared_error(actual[mask], pred[mask]))
                    count = mask.sum()
                    range_name = f"P{percentiles[i]}-P{percentiles[i+1]}"
                    
                    axis_results.append({
                        'range': range_name,
                        'boundaries': (range_boundaries[i], range_boundaries[i+1]),
                        'count': count,
                        'rmse': rmse
                    })
                    
                    print(f"  {axis}è»¸ {range_name}: RMSE={rmse:.4f}, æ¨£æœ¬æ•¸={count}")
            
            ranges_results[f'{axis}_axis'] = axis_results
        
        return ranges_results
    
    def _analyze_by_temperature_range(self, y_pred_X, y_pred_Z):
        """æŒ‰æº«åº¦ç¯„åœåˆ†å€åˆ†æ"""
        temp_results = {}
        
        spindle_temp = self.X_test['Spindle Motor']
        
        # å®šç¾©æº«åº¦å€é–“ (åŸºæ–¼å››åˆ†ä½æ•¸)
        temp_boundaries = np.percentile(spindle_temp, [0, 33, 67, 100])
        temp_labels = ['ä½æº«', 'ä¸­æº«', 'é«˜æº«']
        
        for axis, actual, pred in [('X', self.y_test['Disp. X'], y_pred_X), 
                                   ('Z', self.y_test['Disp. Z'], y_pred_Z)]:
            axis_results = []
            for i, label in enumerate(temp_labels):
                if i < len(temp_labels) - 1:
                    mask = (spindle_temp >= temp_boundaries[i]) & (spindle_temp < temp_boundaries[i+1])
                else:
                    mask = spindle_temp >= temp_boundaries[i]
                
                if mask.sum() > 0:
                    rmse = np.sqrt(mean_squared_error(actual[mask], pred[mask]))
                    count = mask.sum()
                    
                    axis_results.append({
                        'temp_range': label,
                        'boundaries': (temp_boundaries[i], temp_boundaries[i+1] if i < len(temp_boundaries)-1 else temp_boundaries[i]),
                        'count': count,
                        'rmse': rmse
                    })
                    
                    print(f"  {axis}è»¸ {label}å€é–“: RMSE={rmse:.4f}, æ¨£æœ¬æ•¸={count}")
            
            temp_results[f'{axis}_axis'] = axis_results
        
        return temp_results
    
    def _analyze_by_time_range(self, y_pred_X, y_pred_Z):
        """æŒ‰æ™‚é–“åˆ†å€åˆ†æ"""
        time_results = {}
        
        time_data = self.X_test['Time']
        
        # å®šç¾©æ™‚é–“å€é–“
        time_boundaries = np.percentile(time_data, [0, 25, 50, 75, 100])
        time_labels = ['åˆæœŸ', 'å‰æœŸ', 'ä¸­æœŸ', 'å¾ŒæœŸ']
        
        for axis, actual, pred in [('X', self.y_test['Disp. X'], y_pred_X), 
                                   ('Z', self.y_test['Disp. Z'], y_pred_Z)]:
            axis_results = []
            for i, label in enumerate(time_labels):
                if i < len(time_labels) - 1:
                    mask = (time_data >= time_boundaries[i]) & (time_data < time_boundaries[i+1])
                else:
                    mask = time_data >= time_boundaries[i]
                
                if mask.sum() > 0:
                    rmse = np.sqrt(mean_squared_error(actual[mask], pred[mask]))
                    count = mask.sum()
                    
                    axis_results.append({
                        'time_period': label,
                        'boundaries': (time_boundaries[i], time_boundaries[i+1] if i < len(time_boundaries)-1 else time_boundaries[i]),
                        'count': count,
                        'rmse': rmse
                    })
                    
                    print(f"  {axis}è»¸ {label}: RMSE={rmse:.4f}, æ¨£æœ¬æ•¸={count}")
            
            time_results[f'{axis}_axis'] = axis_results
        
        return time_results
    
    def _analyze_extreme_cases(self, y_pred_X, y_pred_Z):
        """æ¥µç«¯å€¼åˆ†æ"""
        extreme_results = {}
        
        for axis, actual, pred in [('X', self.y_test['Disp. X'], y_pred_X), 
                                   ('Z', self.y_test['Disp. Z'], y_pred_Z)]:
            
            # è¨ˆç®—çµ•å°èª¤å·®
            abs_errors = np.abs(actual - pred)
            
            # æ‰¾å‡ºæœ€å¤§èª¤å·®çš„å‰5%æ¨£æœ¬
            error_threshold = np.percentile(abs_errors, 95)
            extreme_mask = abs_errors >= error_threshold
            
            if extreme_mask.sum() > 0:
                extreme_rmse = np.sqrt(mean_squared_error(actual[extreme_mask], pred[extreme_mask]))
                max_error = abs_errors.max()
                extreme_count = extreme_mask.sum()
                
                # æ‰¾å‡ºæ¥µç«¯å€¼çš„ç‰¹å¾µ
                extreme_indices = np.where(extreme_mask)[0]
                extreme_temp = self.X_test['Spindle Motor'].iloc[extreme_indices]
                extreme_time = self.X_test['Time'].iloc[extreme_indices]
                
                extreme_results[f'{axis}_axis'] = {
                    'count': extreme_count,
                    'rmse': extreme_rmse,
                    'max_error': max_error,
                    'avg_temp': extreme_temp.mean(),
                    'avg_time': extreme_time.mean(),
                    'error_threshold': error_threshold
                }
                
                print(f"  {axis}è»¸æ¥µç«¯æƒ…æ³ (èª¤å·®>P95): RMSE={extreme_rmse:.4f}, æœ€å¤§èª¤å·®={max_error:.4f}, æ¨£æœ¬æ•¸={extreme_count}")
                print(f"    æ¥µç«¯æ¨£æœ¬å¹³å‡æº«åº¦: {extreme_temp.mean():.2f}, å¹³å‡æ™‚é–“: {extreme_time.mean():.2f}")
        
        return extreme_results
    
    def plot_predictions(self, y_pred_X, y_pred_Z):
        """ç¹ªè£½é æ¸¬çµæœå°æ¯”åœ–"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # X-axis prediction vs actual
        axes[0, 0].scatter(self.y_test['Disp. X'], y_pred_X, alpha=0.5)
        axes[0, 0].plot([self.y_test['Disp. X'].min(), self.y_test['Disp. X'].max()], 
                        [self.y_test['Disp. X'].min(), self.y_test['Disp. X'].max()], 'r--')
        axes[0, 0].set_xlabel('Actual Disp. X')
        axes[0, 0].set_ylabel('Predicted Disp. X')
        axes[0, 0].set_title('Disp. X Prediction')
        
        # Z-axis prediction vs actual
        axes[0, 1].scatter(self.y_test['Disp. Z'], y_pred_Z, alpha=0.5)
        axes[0, 1].plot([self.y_test['Disp. Z'].min(), self.y_test['Disp. Z'].max()], 
                        [self.y_test['Disp. Z'].min(), self.y_test['Disp. Z'].max()], 'r--')
        axes[0, 1].set_xlabel('Actual Disp. Z')
        axes[0, 1].set_ylabel('Predicted Disp. Z')
        axes[0, 1].set_title('Disp. Z Prediction')
        
        # X residuals
        residuals_X = self.y_test['Disp. X'] - y_pred_X
        axes[1, 0].scatter(y_pred_X, residuals_X, alpha=0.5)
        axes[1, 0].axhline(y=0, color='r', linestyle='--')
        axes[1, 0].set_xlabel('Predicted Disp. X')
        axes[1, 0].set_ylabel('Residual')
        axes[1, 0].set_title('Disp. X Residuals')
        
        # Z residuals
        residuals_Z = self.y_test['Disp. Z'] - y_pred_Z
        axes[1, 1].scatter(y_pred_Z, residuals_Z, alpha=0.5)
        axes[1, 1].axhline(y=0, color='r', linestyle='--')
        axes[1, 1].set_xlabel('Predicted Disp. Z')
        axes[1, 1].set_ylabel('Residual')
        axes[1, 1].set_title('Disp. Z Residuals')
        
        plt.tight_layout()
        plt.savefig('prediction_results.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def feature_importance_analysis(self):
        """ç‰¹å¾µé‡è¦æ€§åˆ†æ"""
        if hasattr(self.model_X, 'feature_importances_'):
            print("\n=== ç‰¹å¾µé‡è¦æ€§åˆ†æ ===")
            importance_X = self.model_X.feature_importances_
            importance_Z = self.model_Z.feature_importances_
            feature_importance_df = pd.DataFrame({
                'Feature': self.feature_columns,
                'Importance_X': importance_X,
                'Importance_Z': importance_Z
            })
            feature_importance_df['Avg_Importance'] = (feature_importance_df['Importance_X'] + feature_importance_df['Importance_Z']) / 2
            feature_importance_df = feature_importance_df.sort_values('Avg_Importance', ascending=False)
            print("å‰15å€‹é‡è¦ç‰¹å¾µ:")
            print(feature_importance_df.head(15))
            plt.figure(figsize=(12, 8))
            top_features = feature_importance_df.head(15)
            x_pos = np.arange(len(top_features))
            plt.barh(x_pos, top_features['Avg_Importance'])
            plt.yticks(x_pos, top_features['Feature'])
            plt.xlabel('Feature Importance')
            plt.title('Top 15 Features')
            plt.gca().invert_yaxis()
            plt.tight_layout()
            plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
            plt.show()
            return feature_importance_df

    def plot_segmented_analysis(self, segmented_results):
        """ç¹ªè£½åˆ†å€åˆ†æçµæœåœ–è¡¨"""
        print("\n=== ç”Ÿæˆåˆ†å€åˆ†æåœ–è¡¨ ===")
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Segmented RMSE Analysis', fontsize=16)
        self._plot_displacement_ranges(axes[0, 0], segmented_results['displacement_ranges'])
        self._plot_temperature_ranges(axes[0, 1], segmented_results['temperature_ranges'])
        self._plot_time_ranges(axes[0, 2], segmented_results['time_ranges'])
        self._plot_extreme_analysis(axes[1, 0], segmented_results['extreme_cases'])
        self._plot_rmse_heatmap(axes[1, 1], segmented_results)
        self._plot_error_distribution(axes[1, 2])
        plt.tight_layout()
        plt.savefig('segmented_rmse_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def _plot_displacement_ranges(self, ax, data):
        """ç¹ªè£½è®Šä½å¤§å°åˆ†å€çµæœ"""
        x_ranges = [item['range'] for item in data['X_axis']]
        x_rmse = [item['rmse'] for item in data['X_axis']]
        z_rmse = [item['rmse'] for item in data['Z_axis']]
        
        x = np.arange(len(x_ranges))
        width = 0.35
        
        ax.bar(x - width/2, x_rmse, width, label='X Axis', alpha=0.8)
        ax.bar(x + width/2, z_rmse, width, label='Z Axis', alpha=0.8)
        
        ax.set_xlabel('Displacement Range')
        ax.set_ylabel('RMSE (Î¼m)')
        ax.set_title('RMSE by Displacement Range')
        ax.set_xticks(x)
        ax.set_xticklabels(x_ranges, rotation=45)
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_temperature_ranges(self, ax, data):
        """ç¹ªè£½æº«åº¦åˆ†å€çµæœ"""
        temp_ranges = [item['temp_range'] for item in data['X_axis']]
        x_rmse = [item['rmse'] for item in data['X_axis']]
        z_rmse = [item['rmse'] for item in data['Z_axis']]
        
        x = np.arange(len(temp_ranges))
        width = 0.35
        
        ax.bar(x - width/2, x_rmse, width, label='X Axis', alpha=0.8)
        ax.bar(x + width/2, z_rmse, width, label='Z Axis', alpha=0.8)
        
        ax.set_xlabel('Temperature Range')
        ax.set_ylabel('RMSE (Î¼m)')
        ax.set_title('RMSE by Temperature Range')
        ax.set_xticks(x)
        ax.set_xticklabels(temp_ranges)
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_time_ranges(self, ax, data):
        """ç¹ªè£½æ™‚é–“åˆ†å€çµæœ"""
        time_ranges = [item['time_period'] for item in data['X_axis']]
        x_rmse = [item['rmse'] for item in data['X_axis']]
        z_rmse = [item['rmse'] for item in data['Z_axis']]
        
        x = np.arange(len(time_ranges))
        width = 0.35
        
        ax.bar(x - width/2, x_rmse, width, label='X Axis', alpha=0.8)
        ax.bar(x + width/2, z_rmse, width, label='Z Axis', alpha=0.8)
        
        ax.set_xlabel('Time Period')
        ax.set_ylabel('RMSE (Î¼m)')
        ax.set_title('RMSE by Time Period')
        ax.set_xticks(x)
        ax.set_xticklabels(time_ranges)
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_extreme_analysis(self, ax, data):
        """ç¹ªè£½æ¥µç«¯å€¼åˆ†æ"""
        axes_names = ['X Axis', 'Z Axis']
        extreme_rmse = [data['X_axis']['rmse'], data['Z_axis']['rmse']]
        max_errors = [data['X_axis']['max_error'], data['Z_axis']['max_error']]
        
        x = np.arange(len(axes_names))
        width = 0.35
        
        ax.bar(x - width/2, extreme_rmse, width, label='Extreme RMSE', alpha=0.8, color='red')
        ax2 = ax.twinx()
        ax2.bar(x + width/2, max_errors, width, label='Max Error', alpha=0.8, color='orange')
        
        ax.set_xlabel('Axis')
        ax.set_ylabel('Extreme RMSE (Î¼m)', color='red')
        ax2.set_ylabel('Max Error (Î¼m)', color='orange')
        ax.set_title('Extreme Error Analysis (Top 5%)')
        ax.set_xticks(x)
        ax.set_xticklabels(axes_names)
        ax.grid(True, alpha=0.3)
    
    def _plot_rmse_heatmap(self, ax, data):
        """ç¹ªè£½RMSEç†±åŠ›åœ–"""
        # å‰µå»ºç†±åŠ›åœ–æ•¸æ“š
        conditions = ['P0-P25', 'P25-P50', 'P50-P75', 'P75-P100']
        axes_names = ['X Axis', 'Z Axis']
        
        rmse_matrix = []
        for axis_key in ['X_axis', 'Z_axis']:
            axis_rmse = [item['rmse'] for item in data['displacement_ranges'][axis_key]]
            rmse_matrix.append(axis_rmse)
        
        im = ax.imshow(rmse_matrix, cmap='YlOrRd', aspect='auto')
        ax.set_xticks(np.arange(len(conditions)))
        ax.set_yticks(np.arange(len(axes_names)))
        ax.set_xticklabels(conditions)
        ax.set_yticklabels(axes_names)
        ax.set_title('RMSE Heatmap')
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for i in range(len(axes_names)):
            for j in range(len(conditions)):
                ax.text(j, i, f'{rmse_matrix[i][j]:.2f}',
                        ha="center", va="center", color="black")
        
        plt.colorbar(im, ax=ax, label='RMSE (Î¼m)')
    
    def _plot_error_distribution(self, ax):
        """ç¹ªè£½èª¤å·®åˆ†å¸ƒåœ–"""
        # é€™å€‹æ–¹æ³•éœ€è¦é æ¸¬çµæœï¼Œæˆ‘å€‘åœ¨ä¸»è¦è©•ä¼°ä¸­èª¿ç”¨
        ax.text(0.5, 0.5, 'Error Distribution\n(Prediction Data Needed)', 
                ha='center', va='center', transform=ax.transAxes)
        ax.set_title('Error Distribution')
        ax.grid(True, alpha=0.3)

def main():
    """ä¸»ç¨‹å¼"""
    # è³‡æ–™è·¯å¾‘
    data_path = r"d:\Github\BigDataCompetition\2025_dataset_0806_x893\2025_dataset_0806\train"
    
    # å»ºç«‹é æ¸¬å™¨
    predictor = ThermalDisplacementPredictor(data_path)
    
    # è¼‰å…¥è³‡æ–™
    data = predictor.load_data()
    
    # è³‡æ–™æ¢ç´¢
    predictor.explore_data()
    
    # æº–å‚™ç‰¹å¾µ
    predictor.prepare_features()
    
    # è™•ç†ç¼ºå¤±è³‡æ–™
    predictor.handle_missing_data()
    
    # åˆ†å‰²è³‡æ–™
    predictor.split_data()
    
    # è¨“ç·´ç·šæ€§å›æ­¸æ¨¡å‹
    print("\n" + "="*50)
    print("è¨“ç·´ç·šæ€§å›æ­¸æ¨¡å‹")
    print("="*50)
    predictor.train_model('linear')
    linear_results = predictor.evaluate_model()
    
    # è¨“ç·´éš¨æ©Ÿæ£®æ—æ¨¡å‹
    print("\n" + "="*50)
    print("è¨“ç·´éš¨æ©Ÿæ£®æ—æ¨¡å‹")
    print("="*50)
    predictor.train_model('random_forest')
    rf_results = predictor.evaluate_model()
    
    # è¨“ç·´æ¢¯åº¦æå‡æ¨¡å‹
    print("\n" + "="*50)
    print("è¨“ç·´æ¢¯åº¦æå‡æ¨¡å‹")
    print("="*50)
    predictor.train_model('gradient_boosting')
    gb_results = predictor.evaluate_model()
    
    # ç‰¹å¾µé‡è¦æ€§åˆ†æ
    predictor.feature_importance_analysis()
    
    # åˆ†å€RMSEåˆ†ææ¯”è¼ƒ
    print("\n" + "="*50)
    print("åˆ†å€RMSEåˆ†ææ¯”è¼ƒ")
    print("="*50)
    
    # é‡æ–°è¼‰å…¥æ¨¡å‹é€²è¡Œåˆ†å€åˆ†æ
    print("\nğŸ” éš¨æ©Ÿæ£®æ—æ¨¡å‹åˆ†å€åˆ†æ:")
    predictor.train_model('random_forest')
    rf_results_detailed = predictor.evaluate_model()
    if 'segmented_results' in rf_results_detailed:
        predictor.plot_segmented_analysis(rf_results_detailed['segmented_results'])
    
    print("\nğŸ” æ¢¯åº¦æå‡æ¨¡å‹åˆ†å€åˆ†æ:")
    predictor.train_model('gradient_boosting')
    gb_results_detailed = predictor.evaluate_model()
    
    print("\nğŸ” ç·šæ€§å›æ­¸æ¨¡å‹åˆ†å€åˆ†æ:")
    predictor.train_model('linear')
    linear_results_detailed = predictor.evaluate_model()
    
    # æ¯”è¼ƒæ¨¡å‹çµæœ
    print("\n" + "="*50)
    print("æ¨¡å‹æ¯”è¼ƒ")
    print("="*50)
    print(f"ç·šæ€§å›æ­¸ - å¹³å‡RMSE: {linear_results['avg_rmse']:.6f}")
    print(f"éš¨æ©Ÿæ£®æ— - å¹³å‡RMSE: {rf_results['avg_rmse']:.6f}")
    print(f"æ¢¯åº¦æå‡ - å¹³å‡RMSE: {gb_results['avg_rmse']:.6f}")
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_rmse = min(linear_results['avg_rmse'], rf_results['avg_rmse'], gb_results['avg_rmse'])
    if best_rmse == linear_results['avg_rmse']:
        print("ç·šæ€§å›æ­¸æ¨¡å‹è¡¨ç¾æœ€å¥½ï¼")
    elif best_rmse == rf_results['avg_rmse']:
        print("éš¨æ©Ÿæ£®æ—æ¨¡å‹è¡¨ç¾æœ€å¥½ï¼")
    else:
        print("æ¢¯åº¦æå‡æ¨¡å‹è¡¨ç¾æœ€å¥½ï¼")

if __name__ == "__main__":
    main()
